{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.SettingWithCopyWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values (NaNs) in the dataset: meta_song.parquet\n",
      "song_id             0\n",
      "artist_id      128866\n",
      "song_length    128866\n",
      "album_id       323497\n",
      "language_id    323497\n",
      "album_month    323523\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Read the Parquet file and drop duplicates based on \"song_id\" column\n",
    "df = pd.read_parquet(\"../data/meta_song.parquet\").drop_duplicates(\"song_id\")\n",
    "# Count the number of missing values (NaNs) in the DataFrame\n",
    "na_count = df.isna().sum()\n",
    "print(\"Number of missing values (NaNs) in the dataset: meta_song.parquet\")\n",
    "print(na_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_source = pd.read_parquet(\"../data/label_train_source.parquet\")\n",
    "train_target = pd.read_parquet(\"../data/label_train_target.parquet\")\n",
    "test_source  = pd.read_parquet(\"../data/label_test_source.parquet\")\n",
    "\n",
    "train_source = train_source.sort_values(by=['session_id', 'listening_order'], ascending=[True, True])\n",
    "train_target = train_target.sort_values(by=['session_id', 'listening_order'], ascending=[True, True])\n",
    "test_source  = test_source.sort_values(by=['session_id', 'listening_order'], ascending=[True, True])\n",
    "\n",
    "def merge_feacture_dataset(df:pd.DataFrame)->pd.DataFrame:\n",
    "    shape = df.shape[0]\n",
    "    # df1 = pd.read_parquet(\"../data/meta_song_composer.parquet\").drop_duplicates(\"song_id\")\n",
    "    # print(len(set(df['song_id'].unique())-set(df1['song_id'].unique())))\n",
    "    # df = pd.merge(df, df1, how='left') \n",
    "    # assert df.shape[0] == shape, f\"origin shape: {shape}, merge after shape: {df.shape[0]}\"\n",
    "    # df2 = pd.read_parquet(\"../data/meta_song_genre.parquet\").drop_duplicates(\"song_id\")\n",
    "    # print(len(set(df['song_id'].unique())-set(df2['song_id'].unique())))\n",
    "    # df = pd.merge(df, df2, how='left') \n",
    "    # assert df.shape[0] == shape, f\"origin shape: {shape}, merge after shape: {df.shape[0]}\"\n",
    "    # df3 = pd.read_parquet(\"../data/meta_song_lyricist.parquet\").drop_duplicates(\"song_id\")\n",
    "    # print(len(set(df['song_id'].unique())-set(df3['song_id'].unique())))\n",
    "    # df = pd.merge(df, df3, how='left') \n",
    "    # assert df.shape[0] == shape, f\"origin shape: {shape}, merge after shape: {df.shape[0]}\"\n",
    "    # df4 = pd.read_parquet(\"../data/meta_song_producer.parquet\").drop_duplicates(\"song_id\")\n",
    "    # print(len(set(df['song_id'].unique())-set(df4['song_id'].unique())))\n",
    "    # df = pd.merge(df, df4, how='left') \n",
    "    # assert df.shape[0] == shape, f\"origin shape: {shape}, merge after shape: {df.shape[0]}\"\n",
    "    # df5 = pd.read_parquet(\"../data/meta_song_titletext.parquet\").drop_duplicates(\"song_id\")\n",
    "    # print(len(set(df['song_id'].unique())-set(df5['song_id'].unique())))\n",
    "    # df = pd.merge(df, df5, how='left') \n",
    "    # assert df.shape[0] == shape, f\"origin shape: {shape}, merge after shape: {df.shape[0]}\"\n",
    "    df6 = pd.read_parquet(\"../data/meta_song.parquet\").drop_duplicates(\"song_id\")\n",
    "    print(len(set(df['song_id'].unique())-set(df6['song_id'].unique())))\n",
    "    df = pd.merge(df, df6, how='left') \n",
    "    assert df.shape[0] == shape, f\"origin shape: {shape}, merge after shape: {df.shape[0]}\"\n",
    "    print(f\"Merge finish!, now shape is : {df.shape}\")\n",
    "    return df\n",
    "\n",
    "#train_source = merge_feacture_dataset(train_source)\n",
    "#train_target = merge_feacture_dataset(train_target)\n",
    "#test_source  = merge_feacture_dataset(test_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode the feactures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>song_id</th>\n",
       "      <th>unix_played_at</th>\n",
       "      <th>play_status</th>\n",
       "      <th>login_type</th>\n",
       "      <th>listening_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10952316</th>\n",
       "      <td>1</td>\n",
       "      <td>f6f06a71bb8bc38af6c0b7dae9cab00d</td>\n",
       "      <td>1660012505</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10952317</th>\n",
       "      <td>1</td>\n",
       "      <td>7b48a87effd31c9c07b68ed212062854</td>\n",
       "      <td>1660012730</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10952318</th>\n",
       "      <td>1</td>\n",
       "      <td>61c46d6401aab1dde7c7de23dc55c037</td>\n",
       "      <td>1660015113</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10952319</th>\n",
       "      <td>1</td>\n",
       "      <td>7e54c9199aad70e35fe256d23701bad0</td>\n",
       "      <td>1660015289</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10952320</th>\n",
       "      <td>1</td>\n",
       "      <td>6178580fa01b62e9b52787902c0d8ae6</td>\n",
       "      <td>1660015841</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10952321</th>\n",
       "      <td>1</td>\n",
       "      <td>ab694649c65477d0bc574bf391a3f4a0</td>\n",
       "      <td>1660015842</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10952322</th>\n",
       "      <td>1</td>\n",
       "      <td>5b3387fa195672dcfe979d17e4a62c9e</td>\n",
       "      <td>1660015846</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10952323</th>\n",
       "      <td>1</td>\n",
       "      <td>2790c612d8d301e2f35550c75aea8c75</td>\n",
       "      <td>1660015846</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10952324</th>\n",
       "      <td>1</td>\n",
       "      <td>d36c6cf30154e18e6c972704206d6b1e</td>\n",
       "      <td>1660015848</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10952325</th>\n",
       "      <td>1</td>\n",
       "      <td>1cbcc681ecf7acef4948bff2eb8e39d7</td>\n",
       "      <td>1660015850</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10952326</th>\n",
       "      <td>1</td>\n",
       "      <td>95eb6b55a0b6d049aadd729aaabd63de</td>\n",
       "      <td>1660015852</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10952327</th>\n",
       "      <td>1</td>\n",
       "      <td>7035839edc259dcad4b1632d10eded74</td>\n",
       "      <td>1660015853</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10952328</th>\n",
       "      <td>1</td>\n",
       "      <td>420af27f7145b4eebeec566c0fa7a4c1</td>\n",
       "      <td>1660015855</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10952329</th>\n",
       "      <td>1</td>\n",
       "      <td>ea3083c238e4fbb01a8035816ad7101f</td>\n",
       "      <td>1660015857</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10952330</th>\n",
       "      <td>1</td>\n",
       "      <td>e060390d1cfa800bd6032cfa524c57f6</td>\n",
       "      <td>1660015859</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10952331</th>\n",
       "      <td>1</td>\n",
       "      <td>43fdd8f154e5c522eef60f6edfb38896</td>\n",
       "      <td>1660015860</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10952332</th>\n",
       "      <td>1</td>\n",
       "      <td>8d1ee4d9df7226fd8af3c4466a48afdc</td>\n",
       "      <td>1660015862</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10952333</th>\n",
       "      <td>1</td>\n",
       "      <td>2ad3043e1a7e459ddb09c5ba27e475f8</td>\n",
       "      <td>1660015863</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10952334</th>\n",
       "      <td>1</td>\n",
       "      <td>7bb8fadfc8f2bf145f4b29a0325fe79a</td>\n",
       "      <td>1660016101</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10952335</th>\n",
       "      <td>1</td>\n",
       "      <td>824c159701c8553b0e38f0d36ddd6197</td>\n",
       "      <td>1660016310</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          session_id                           song_id  unix_played_at  \\\n",
       "10952316           1  f6f06a71bb8bc38af6c0b7dae9cab00d      1660012505   \n",
       "10952317           1  7b48a87effd31c9c07b68ed212062854      1660012730   \n",
       "10952318           1  61c46d6401aab1dde7c7de23dc55c037      1660015113   \n",
       "10952319           1  7e54c9199aad70e35fe256d23701bad0      1660015289   \n",
       "10952320           1  6178580fa01b62e9b52787902c0d8ae6      1660015841   \n",
       "10952321           1  ab694649c65477d0bc574bf391a3f4a0      1660015842   \n",
       "10952322           1  5b3387fa195672dcfe979d17e4a62c9e      1660015846   \n",
       "10952323           1  2790c612d8d301e2f35550c75aea8c75      1660015846   \n",
       "10952324           1  d36c6cf30154e18e6c972704206d6b1e      1660015848   \n",
       "10952325           1  1cbcc681ecf7acef4948bff2eb8e39d7      1660015850   \n",
       "10952326           1  95eb6b55a0b6d049aadd729aaabd63de      1660015852   \n",
       "10952327           1  7035839edc259dcad4b1632d10eded74      1660015853   \n",
       "10952328           1  420af27f7145b4eebeec566c0fa7a4c1      1660015855   \n",
       "10952329           1  ea3083c238e4fbb01a8035816ad7101f      1660015857   \n",
       "10952330           1  e060390d1cfa800bd6032cfa524c57f6      1660015859   \n",
       "10952331           1  43fdd8f154e5c522eef60f6edfb38896      1660015860   \n",
       "10952332           1  8d1ee4d9df7226fd8af3c4466a48afdc      1660015862   \n",
       "10952333           1  2ad3043e1a7e459ddb09c5ba27e475f8      1660015863   \n",
       "10952334           1  7bb8fadfc8f2bf145f4b29a0325fe79a      1660016101   \n",
       "10952335           1  824c159701c8553b0e38f0d36ddd6197      1660016310   \n",
       "\n",
       "          play_status  login_type  listening_order  \n",
       "10952316            0           7                1  \n",
       "10952317            0           7                2  \n",
       "10952318            0           7                3  \n",
       "10952319            0           7                4  \n",
       "10952320            0           7                5  \n",
       "10952321            0           7                6  \n",
       "10952322            0           7                7  \n",
       "10952323            0           7                8  \n",
       "10952324            0           7                9  \n",
       "10952325            0           7               10  \n",
       "10952326            0           7               11  \n",
       "10952327            0           7               12  \n",
       "10952328            0           7               13  \n",
       "10952329            0           7               14  \n",
       "10952330            0           7               15  \n",
       "10952331            0           7               16  \n",
       "10952332            0           7               17  \n",
       "10952333            0           7               18  \n",
       "10952334            0           7               19  \n",
       "10952335            0           7               20  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_source.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_NaNs(df:pd.DataFrame, numerical_columns:list=None, string_columns:list=None)->pd.DataFrame:\n",
    "    for column in numerical_columns:\n",
    "        df[column].fillna(df[column].mean(), inplace=True)\n",
    "    for column in string_columns:\n",
    "        df[column].fillna(0)\n",
    "    return df\n",
    "\n",
    "def encode_unix_time(df:pd.DataFrame, sin_cos = False):\n",
    "    # Convert 'unix_played_at' to a datetime column\n",
    "    df['played_at_datetime'] = pd.to_datetime(df['unix_played_at'], unit='s')\n",
    "    if sin_cos:\n",
    "        df['hour_sin'] = np.sin(2 * np.pi * df['played_at_datetime'].dt.hour / 24)\n",
    "        df['hour_cos'] = np.cos(2 * np.pi * df['played_at_datetime'].dt.hour / 24)\n",
    "        df['minute_sin'] = np.sin(2 * np.pi * df['played_at_datetime'].dt.minute / 60)\n",
    "        df['minute_cos'] = np.cos(2 * np.pi * df['played_at_datetime'].dt.minute / 60)\n",
    "        df['second_sin'] = np.sin(2 * np.pi * df['played_at_datetime'].dt.second / 60)\n",
    "        df['second_cos'] = np.cos(2 * np.pi * df['played_at_datetime'].dt.second / 60)\n",
    "        df['month_sin'] = np.sin(2 * np.pi * df['played_at_datetime'].dt.month / 12)\n",
    "        df['month_cos'] = np.cos(2 * np.pi * df['played_at_datetime'].dt.month / 12)\n",
    "        df['year_sin'] = np.sin(2 * np.pi * df['played_at_datetime'].dt.year / 2023)\n",
    "        df['year_cos'] = np.cos(2 * np.pi * df['played_at_datetime'].dt.year / 2023)\n",
    "    else:\n",
    "        df['hour_of_day'] = df['played_at_datetime'].dt.hour / 24\n",
    "        df['minute_of_hour'] = df['played_at_datetime'].dt.minute / 60\n",
    "        df['second_of_minute'] = df['played_at_datetime'].dt.second / 60\n",
    "        df['month'] = df['played_at_datetime'].dt.month / 12\n",
    "        df['year'] = df['played_at_datetime'].dt.year / 2023\n",
    "    # Drop the specified columns from the DataFrame\n",
    "    df.drop(columns=['unix_played_at', 'played_at_datetime'], inplace=True)\n",
    "    return df\n",
    "\n",
    "def get_song_ID_encode_dict(train:pd.DataFrame, test:pd.DataFrame)->dict:\n",
    "    unique_song_ids = set(train['song_id'].tolist()+test['song_id'].tolist())\n",
    "    ID_IDX = {song_id:i+1 for i,song_id in enumerate(unique_song_ids)}\n",
    "    ID_IDX[\"SOS\"]=0\n",
    "    return ID_IDX\n",
    "\n",
    "def encode_song_id(source_df:pd.DataFrame, target_df:pd.DataFrame, id2idx:dict):\n",
    "    source_df['song_id'] = source_df['song_id'].map(id2idx)\n",
    "    target_df['song_id'] = target_df['song_id'].map(id2idx)\n",
    "    return source_df, target_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_IDX = get_song_ID_encode_dict(train_source, test_source)\n",
    "train_source, train_target = encode_song_id(train_source, train_target, ID_IDX)\n",
    "train_source = encode_unix_time(train_source)\n",
    "train_target = encode_unix_time(train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4608/11445180 [00:00<08:22, 22777.99it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11445180/11445180 [08:37<00:00, 22104.61it/s]\n",
      "100%|██████████| 2861295/2861295 [00:31<00:00, 91232.28it/s]\n"
     ]
    }
   ],
   "source": [
    "def convert_per_N(df:pd.DataFrame, n:int, label = False):\n",
    "    data = []\n",
    "    pre_session_id = int(df['session_id'].iloc[0])\n",
    "    if label:\n",
    "        row = [0]\n",
    "        for i in tqdm(range(df.shape[0])):\n",
    "            # next session id\n",
    "            if pre_session_id != int(df['session_id'].iloc[i]):\n",
    "                data.append((pre_session_id, np.array(row).reshape(-1, 6)))\n",
    "                pre_session_id, row = int(df['session_id'].iloc[i]), [0]\n",
    "            # append 5 values\n",
    "            row.append(df['song_id'].iloc[i])\n",
    "        # append last session id\n",
    "        data.append((df['session_id'].iloc[-1], np.array(row).reshape(-1, 6))) #last one\n",
    "    else:\n",
    "        row = []\n",
    "        song_id = []\n",
    "        for i in tqdm(range(df.shape[0])):\n",
    "            # next session id\n",
    "            if pre_session_id != int(df['session_id'].iloc[i]):\n",
    "                data.append((pre_session_id, \n",
    "                             np.array(row).reshape(-1, 20), \n",
    "                             np.array(song_id).reshape(-1, 20)))\n",
    "                pre_session_id, row,song_id = int(df['session_id'].iloc[i]), [], []\n",
    "            # append 20 values\n",
    "            song_id.append(df['song_id'].iloc[i])\n",
    "            row.append([df['play_status'].iloc[i],\n",
    "                        df['login_type'].iloc[i],\n",
    "                        df['second_of_minute'].iloc[i],\n",
    "                        df['minute_of_hour'].iloc[i],\n",
    "                        df['hour_of_day'].iloc[i],\n",
    "                        df['month'].iloc[i], \n",
    "                        df['year'].iloc[i]])\n",
    "        # append last session id\n",
    "        data.append((df['session_id'].iloc[-1], \n",
    "                     np.array(row).reshape(-1, 20),\n",
    "                     np.array(song_id).reshape(-1, 20)))\n",
    "    return data\n",
    "\n",
    "train_source_data  = convert_per_N(train_source, 20)\n",
    "train_source_label = convert_per_N(train_target, 5, label=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convert to dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RankingDatset(Dataset):\n",
    "    def __init__(self, data, label):\n",
    "        self.session_id = [session_id for session_id,_,_ in data]\n",
    "        self.feature    = [feature    for _,feature,_    in data]\n",
    "        self.song_id    = [song_id    for _,_,song_id    in data]\n",
    "        self.label      = [label for _,label in label]\n",
    "    def __len__(self):\n",
    "        return len(self.session_id)\n",
    "    def __getitem__(self, idx):\n",
    "        session_id = self.session_id[idx]\n",
    "        feature = torch.tensor(self.feature[idx], dtype=torch.float32)\n",
    "        song_id = torch.tensor(self.song_id[idx], dtype=torch.float32)\n",
    "        label = torch.tensor(self.label[idx], dtype=torch.float32)\n",
    "        return {'session_id': session_id, 'feature': feature, 'song_id': song_id, 'label': label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'session_id': 1,\n",
       " 'feature': tensor([[0.0000, 7.0000, 0.0833, 0.5833, 0.0833, 0.6667, 0.9995, 0.0000, 7.0000,\n",
       "          0.8333, 0.6333, 0.0833, 0.6667, 0.9995, 0.0000, 7.0000, 0.5500, 0.3000,\n",
       "          0.1250, 0.6667],\n",
       "         [0.9995, 0.0000, 7.0000, 0.4833, 0.3500, 0.1250, 0.6667, 0.9995, 0.0000,\n",
       "          7.0000, 0.6833, 0.5000, 0.1250, 0.6667, 0.9995, 0.0000, 7.0000, 0.7000,\n",
       "          0.5000, 0.1250],\n",
       "         [0.6667, 0.9995, 0.0000, 7.0000, 0.7667, 0.5000, 0.1250, 0.6667, 0.9995,\n",
       "          0.0000, 7.0000, 0.7667, 0.5000, 0.1250, 0.6667, 0.9995, 0.0000, 7.0000,\n",
       "          0.8000, 0.5000],\n",
       "         [0.1250, 0.6667, 0.9995, 0.0000, 7.0000, 0.8333, 0.5000, 0.1250, 0.6667,\n",
       "          0.9995, 0.0000, 7.0000, 0.8667, 0.5000, 0.1250, 0.6667, 0.9995, 0.0000,\n",
       "          7.0000, 0.8833],\n",
       "         [0.5000, 0.1250, 0.6667, 0.9995, 0.0000, 7.0000, 0.9167, 0.5000, 0.1250,\n",
       "          0.6667, 0.9995, 0.0000, 7.0000, 0.9500, 0.5000, 0.1250, 0.6667, 0.9995,\n",
       "          0.0000, 7.0000],\n",
       "         [0.9833, 0.5000, 0.1250, 0.6667, 0.9995, 0.0000, 7.0000, 0.0000, 0.5167,\n",
       "          0.1250, 0.6667, 0.9995, 0.0000, 7.0000, 0.0333, 0.5167, 0.1250, 0.6667,\n",
       "          0.9995, 0.0000],\n",
       "         [7.0000, 0.0500, 0.5167, 0.1250, 0.6667, 0.9995, 0.0000, 7.0000, 0.0167,\n",
       "          0.5833, 0.1250, 0.6667, 0.9995, 0.0000, 7.0000, 0.5000, 0.6333, 0.1250,\n",
       "          0.6667, 0.9995]]),\n",
       " 'song_id': tensor([[ 49291., 513133.,  27995., 502263., 518079., 573134., 691018., 467003.,\n",
       "          449773.,  73311., 230227., 525149., 337491., 396185.,  77569., 304031.,\n",
       "          683175., 243592., 204715., 313681.]]),\n",
       " 'label': tensor([[     0.,   4719., 185344., 334352., 171744., 170711.]])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = RankingDatset(train_source_data, train_source_label)\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, batch_size, num_songs, hidden_size, num_feature, embedding_dim, num_layers=1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.embedding = nn.Embedding(num_songs, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim+num_feature, hidden_size, num_layers, batch_first=True)\n",
    "\n",
    "    def forward(self, song_ids, feature):\n",
    "        embedded = self.embedding(song_ids) \n",
    "        # input feature cat with song embed\n",
    "        lstm_input = torch.cat((embedded, feature.view(self.batch_size,20,-1)), dim=2)  # Concatenate along the feature dimension\n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(lstm_input)  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        return out\n",
    "\n",
    "# Define the Decoder\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, batch_size, num_songs, embedding_dim, enc_hidden_size, hidden_size, num_layers=1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_songs, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim+enc_hidden_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc1 = nn.Linear(20, 1)\n",
    "        self.fc2 = nn.Linear(hidden_size, num_songs)\n",
    "\n",
    "    def forward(self, decode_song_ids, last_hidden, encoder_hidden):\n",
    "        # Forward propagate LSTM\n",
    "        embedded = self.embedding(decode_song_ids) \n",
    "        encoder_hidden = self.fc1(encoder_hidden.view(self.batch_size,-1,20))\n",
    "        lstm_input = torch.cat((embedded, encoder_hidden.view(self.batch_size,1,-1)), dim=2)\n",
    "        out, lstm_hidden = self.lstm(lstm_input, last_hidden)\n",
    "        out = self.fc2(out.squeeze(1))\n",
    "        return out, lstm_hidden\n",
    "\n",
    "# Seq2Seq model\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, song_ids, features, target_song_ids, teacher_forcing_ratio=0.5):\n",
    "        encoder_output = self.encoder(song_ids, features)\n",
    "        \n",
    "        batch_size = target_song_ids.size(0)\n",
    "        target_len = target_song_ids.size(1)\n",
    "        target_song_size = self.decoder.fc2.out_features\n",
    "        outputs = torch.zeros(batch_size, target_len, target_song_size)\n",
    "\n",
    "        decoder_input = target_song_ids[:, 0]  # SOS token as the first input\n",
    "        decoder_hidden = encoder_output[:, -1, :].unsqueeze(0), torch.zeros_like(encoder_output[:, -1, :]).unsqueeze(0)\n",
    "        for t in range(1, target_len):\n",
    "            decoder_output, decoder_hidden = self.decoder(decoder_input.unsqueeze(1), decoder_hidden, encoder_output)\n",
    "            outputs[:, t] = decoder_output\n",
    "            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
    "            top1 = decoder_output.max(1)[1]\n",
    "            decoder_input = target_song_ids[:, t] if teacher_force and t < target_len else top1\n",
    "\n",
    "        return torch.argmax(outputs.squeeze(0),dim = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([     0, 109169, 320771, 220825, 362916, 265796])\n"
     ]
    }
   ],
   "source": [
    "# Create the Encoder and Decoder instances\n",
    "num_songs = len(list(ID_IDX.values()))\n",
    "embedding_dim = 128\n",
    "hidden_size = 256\n",
    "enc_hidden_size = hidden_size\n",
    "dec_hidden_size = hidden_size\n",
    "num_layers = 1\n",
    "batch_size = 1 # 1 for testing\n",
    "num_features = dataset[0]['feature'].shape[0]\n",
    "\n",
    "# Create Encoder and Decoder instances\n",
    "encoder = Encoder(batch_size, num_songs, enc_hidden_size, num_features, embedding_dim, num_layers)\n",
    "decoder = Decoder(batch_size, num_songs, embedding_dim, enc_hidden_size, dec_hidden_size, num_layers)\n",
    "seq2seq_model = Seq2Seq(encoder, decoder)\n",
    "\n",
    "# Sample data\n",
    "data = {\n",
    "    'session_id': 1,\n",
    "    'feature': dataset[0]['feature'].unsqueeze(0),  # tensor data for features\n",
    "    'song_id': dataset[0]['song_id'],\n",
    "    'label': dataset[0]['label'] # 0 is for the start input for decoder\n",
    "}\n",
    "\n",
    "# Assuming you want to use this data for prediction:\n",
    "song_ids = data['song_id'].to(torch.long)\n",
    "features = data['feature'].to(torch.long)\n",
    "target   = data['label'].to(torch.long)\n",
    "# Set the model to evaluation mode\n",
    "seq2seq_model.eval()\n",
    "with torch.no_grad():\n",
    "    predicted_output = seq2seq_model(song_ids, features, target)\n",
    "    \n",
    "# Print the predicted output (just for demonstration)\n",
    "print(predicted_output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
