{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pytorch_lightning as pl\n",
    "import warnings\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.SettingWithCopyWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data and Merge Feacture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = LabelEncoder()\n",
    "\n",
    "def merge_feacture_dataset(df:pd.DataFrame)->pd.DataFrame:\n",
    "    shape = df.shape[0]\n",
    "    df1 = pd.read_parquet(\"../data/meta_song_composer.parquet\").drop_duplicates(\"song_id\")\n",
    "    df1['composer_id'] = enc.fit_transform(df1['composer_id'].fillna('nan'))\n",
    "    #print(len(set(df['song_id'].unique())-set(df1['song_id'].unique())))\n",
    "    df = pd.merge(df, df1, how='left') \n",
    "    assert df.shape[0] == shape, f\"origin shape: {shape}, merge after shape: {df.shape[0]}\"\n",
    "    df2 = pd.read_parquet(\"../data/meta_song_genre.parquet\").drop_duplicates(\"song_id\")\n",
    "    df2['genre_id'] = enc.fit_transform(df2['genre_id'].fillna('nan'))\n",
    "    #print(len(set(df['song_id'].unique())-set(df2['song_id'].unique())))\n",
    "    df = pd.merge(df, df2, how='left') \n",
    "    assert df.shape[0] == shape, f\"origin shape: {shape}, merge after shape: {df.shape[0]}\"\n",
    "    df3 = pd.read_parquet(\"../data/meta_song_lyricist.parquet\").drop_duplicates(\"song_id\")\n",
    "    df3['lyricist_id'] = enc.fit_transform(df3['lyricist_id'].fillna('nan'))\n",
    "    #print(len(set(df['song_id'].unique())-set(df3['song_id'].unique())))\n",
    "    df = pd.merge(df, df3, how='left') \n",
    "    assert df.shape[0] == shape, f\"origin shape: {shape}, merge after shape: {df.shape[0]}\"\n",
    "    df4 = pd.read_parquet(\"../data/meta_song_producer.parquet\").drop_duplicates(\"song_id\")\n",
    "    df4['producer_id'] = enc.fit_transform(df4['producer_id'].fillna('nan'))\n",
    "    #print(len(set(df['song_id'].unique())-set(df4['song_id'].unique())))\n",
    "    df = pd.merge(df, df4, how='left') \n",
    "    assert df.shape[0] == shape, f\"origin shape: {shape}, merge after shape: {df.shape[0]}\"\n",
    "    df5 = pd.read_parquet(\"../data/meta_song_titletext.parquet\").drop_duplicates(\"song_id\")\n",
    "    df5['title_text_id'] = enc.fit_transform(df5['title_text_id'].fillna('nan'))\n",
    "    #print(len(set(df['song_id'].unique())-set(df5['song_id'].unique())))\n",
    "    df = pd.merge(df, df5, how='left') \n",
    "    assert df.shape[0] == shape, f\"origin shape: {shape}, merge after shape: {df.shape[0]}\"\n",
    "    df6 = pd.read_parquet(\"../data/meta_song.parquet\").drop_duplicates(\"song_id\")\n",
    "    df6['artist_id'] = enc.fit_transform(df6['artist_id'].fillna(float(-1)))\n",
    "    df6['album_id'] = enc.fit_transform(df6['album_id'].fillna(float(-1)))\n",
    "    df6['language_id'] = enc.fit_transform(df6['language_id'].fillna(float(-1)))\n",
    "    #print(len(set(df['song_id'].unique())-set(df6['song_id'].unique())))\n",
    "    df = pd.merge(df, df6, how='left') \n",
    "    assert df.shape[0] == shape, f\"origin shape: {shape}, merge after shape: {df.shape[0]}\"\n",
    "    print(f\"Merge finish!, now shape is : {df.shape}\")\n",
    "    return df\n",
    "\n",
    "# load from directory\n",
    "train_source = pd.read_parquet(\"../data/label_train_source.parquet\")\n",
    "train_target = pd.read_parquet(\"../data/label_train_target.parquet\")\n",
    "test_source  = pd.read_parquet(\"../data/label_test_source.parquet\")\n",
    "# sort the data by session_id, listening_order\n",
    "train_source = train_source.sort_values(by=['session_id', 'listening_order'], ascending=[True, True])\n",
    "train_target = train_target.sort_values(by=['session_id', 'listening_order'], ascending=[True, True])\n",
    "test_source  = test_source.sort_values( by=['session_id', 'listening_order'], ascending=[True, True])\n",
    "\n",
    "train_source = merge_feacture_dataset(train_source)\n",
    "train_target = merge_feacture_dataset(train_target)\n",
    "test_source  = merge_feacture_dataset(test_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_song_ID_encode_dict(train_source:pd.DataFrame, train_target:pd.DataFrame, test_source:pd.DataFrame)->dict:\n",
    "    unique_song_ids = set(train_source['song_id'].tolist() +\n",
    "                          train_target['song_id'].tolist() +\n",
    "                          test_source['song_id'].tolist())\n",
    "    ID_to_IDX = {song_id:i+1 for i,song_id in enumerate(unique_song_ids)}\n",
    "    ID_to_IDX[\"SOS\"]=0\n",
    "    IDX_to_ID = {v: k for k, v in ID_to_IDX.items()}\n",
    "    return ID_to_IDX, IDX_to_ID\n",
    "\n",
    "def preprocessing(df:pd.DataFrame, label_encoder:LabelEncoder, ID_to_IDX:dict)->pd.DataFrame:\n",
    "    # IDs Data\n",
    "    for col in ['composer_id', 'genre_id', 'lyricist_id', 'producer_id', 'title_text_id']:\n",
    "        df[col] = df[col]+1\n",
    "        df[col].fillna(0, inplace=True)\n",
    "    for col in ['play_status', 'login_type']:\n",
    "        df[col] = label_encoder.fit_transform(df[col].fillna('nan'))\n",
    "    # numerical data\n",
    "    df['song_length'] = pd.to_numeric(df['song_length'], errors='coerce')\n",
    "    df['song_length'].fillna(df['song_length'].mean(), inplace=True)\n",
    "    # time data\n",
    "    # unix time\n",
    "    df['unix_played_at'] = pd.to_datetime(df['unix_played_at'], unit='s')\n",
    "    df['unix_second'] = df['unix_played_at'].dt.second\n",
    "    df['unix_minute'] = df['unix_played_at'].dt.minute\n",
    "    df['unix_hour']   = df['unix_played_at'].dt.hour\n",
    "    df['unix_month']  = df['unix_played_at'].dt.month\n",
    "    df['unix_year']   = df['unix_played_at'].dt.year\n",
    "    # album month\n",
    "    df['album_month'] = pd.to_datetime(df['album_month'], errors='coerce')\n",
    "    # Calculate mean or median date\n",
    "    mean_date = df['album_month'].mean()  # Use mean() or median() as needed\n",
    "    # Fill NaNs with the mean or median date\n",
    "    df['album_month'].fillna(mean_date, inplace=True)\n",
    "    df['album_Month'] = df['album_month'].dt.month\n",
    "    df['album_Year']  = df['album_month'].dt.year\n",
    "    df.drop(columns=['unix_played_at', 'album_month'], inplace=True) # , 'album_month'\n",
    "    # Song ID encode\n",
    "    df['song_id'] = df['song_id'].map(ID_to_IDX)\n",
    "\n",
    "    return df\n",
    "\n",
    "ID_to_IDX, IDX_to_ID = get_song_ID_encode_dict(train_source, train_target, test_source)\n",
    "train_source = preprocessing(train_source, enc, ID_to_IDX)\n",
    "train_target = preprocessing(train_target, enc, ID_to_IDX)\n",
    "test_source  = preprocessing(test_source,  enc, ID_to_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build sequential Data format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sequential_data(df:pd.DataFrame, for_target:bool=False)->(list, list):\n",
    "    session_list, feature_list = [], []\n",
    "    grouped_sorted_data = df.groupby('session_id')\n",
    "    if for_target:\n",
    "        for session_id, grouped_df in tqdm(grouped_sorted_data):\n",
    "            grouped_df = grouped_df.sort_values(by='listening_order')\n",
    "            session_list.append(session_id)\n",
    "            feature_list.append(grouped_df['song_id'].to_numpy().T)\n",
    "    else:\n",
    "        for session_id, grouped_df in tqdm(grouped_sorted_data):\n",
    "            grouped_df = grouped_df.sort_values(by='listening_order')\n",
    "            grouped_df.drop(columns=['session_id', 'listening_order'], inplace=True)\n",
    "            session_list.append(session_id)\n",
    "            feature_list.append(grouped_df.to_numpy().T)\n",
    "    return session_list, feature_list\n",
    "\n",
    "train_source_session_list, train_source_feature_list = build_sequential_data(train_source)\n",
    "train_target_session_list, train_target_feature_list = build_sequential_data(train_target, for_target=True)\n",
    "test_source_session_list,  test_source_feature_list  = build_sequential_data(test_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperParameters:\n",
    "    def __init__(self, batch_size=512, \n",
    "                 learning_rate=0.001, \n",
    "                 epochs=10, # 1 for testing\n",
    "                 hidden_size = 256,\n",
    "                 embedding_dim = 256,\n",
    "                 lstm_num_layers = 1,\n",
    "                 train_size = 0.8,\n",
    "                 device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "                 save_dir = \"../model/encoder-decoder/\"\n",
    "                 ):\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.lstm_num_layers = lstm_num_layers\n",
    "        self.train_size = train_size\n",
    "        self.device = device\n",
    "        self.save_dir = save_dir\n",
    "\n",
    "    def get_batch_size(self):\n",
    "        return self.batch_size\n",
    "\n",
    "    def get_learning_rate(self):\n",
    "        return self.learning_rate\n",
    "\n",
    "    def get_epochs(self):\n",
    "        return self.epochs\n",
    "\n",
    "    def get_hidden_size(self):\n",
    "        return self.hidden_size\n",
    "    \n",
    "    def get_embedding_dim(self):\n",
    "        return self.embedding_dim\n",
    "    \n",
    "    def get_lstm_num_layers(self):\n",
    "        return self.lstm_num_layers\n",
    "    \n",
    "    def get_train_size(self):\n",
    "        return self.train_size\n",
    "    \n",
    "    def get_device(self):\n",
    "        return self.device\n",
    "    \n",
    "    def get_save_dir(self):\n",
    "        return self.save_dir\n",
    "    \n",
    "hyperparams = HyperParameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RankingDatset(Dataset):\n",
    "    def __init__(self, session_id, feature, label=None, train = True):\n",
    "        self.session_id = session_id\n",
    "        self.feature    = feature\n",
    "        self.label      = label if train else None\n",
    "    def __len__(self):\n",
    "        return len(self.session_id)\n",
    "    def __getitem__(self, idx):\n",
    "        session_id = self.session_id[idx]\n",
    "        feature = torch.tensor(self.feature[idx], dtype=torch.float) \n",
    "        label = torch.tensor(self.label[idx], dtype=torch.long)\n",
    "        return {'session_id': session_id, 'feature': feature, 'label': label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = RankingDatset(train_source_session_list, train_source_feature_list, train_target_feature_list)\n",
    "train_size = int(hyperparams.get_train_size() * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
    "test_dataset  = RankingDatset(test_source_session_list, test_source_feature_list, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=hyperparams.get_batch_size(), shuffle=True)\n",
    "val_dataloader   = DataLoader(val_dataset, batch_size=hyperparams.get_batch_size())\n",
    "test_dataloader  = DataLoader(test_dataset, batch_size=hyperparams.get_batch_size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, hidden_size, num_feature, num_layers=1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers  = num_layers\n",
    "        self.lstm = nn.LSTM(num_feature, hidden_size, num_layers, batch_first=True)\n",
    "\n",
    "    def forward(self, feature):\n",
    "        # feature : torch.Size([batch size, 19, 20])\n",
    "        # Forward propagate LSTM\n",
    "        out, hidden = self.lstm(feature.transpose(1,2))  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "\n",
    "        return out, hidden\n",
    "\n",
    "# Define the Decoder\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_songs, embedding_dim, enc_hidden_size, hidden_size, num_layers=1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_songs, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim+enc_hidden_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc1 = nn.Linear(20, 1)\n",
    "        self.fc2 = nn.Linear(hidden_size, num_songs)\n",
    "\n",
    "    def forward(self, decode_song_ids, last_hidden, encoder_hidden):\n",
    "        # decode_song_ids: torch.Size([batch size, 1])\n",
    "        # encoder_hidden:  torch.Size([batch size, 20, encoder_hidden])\n",
    "        embedded = self.embedding(decode_song_ids).squeeze(1) # torch.Size([batch size, 1, embedding_dim])\n",
    "        encoder_hidden = self.fc1(encoder_hidden.transpose(1,2)) # torch.Size([batch size, enc_hidden_size, 1])\n",
    "        lstm_input = torch.cat((embedded, encoder_hidden.transpose(1,2)), dim=2) # torch.Size([batch size, 1, embedding_dim + enc_hidden_size])\n",
    "        # Forward propagate LSTM\n",
    "        out, lstm_hidden = self.lstm(lstm_input, last_hidden)\n",
    "        out = self.fc2(out.squeeze(1)) # torch.Size([batch size, num_songs])\n",
    "        return out, lstm_hidden\n",
    "\n",
    "# Seq2Seq model\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, features, target_song_ids=None, use_teacher_forcing=True):\n",
    "        encoder_output, hidden = self.encoder(features)\n",
    "        batch_size = features.shape[0]  # 64\n",
    "        target_song_ids = torch.zeros(batch_size, 5).long().to(hyperparams.get_device()) if target_song_ids is None else target_song_ids.squeeze(1)\n",
    "        target_len = target_song_ids.size(-1) # 5\n",
    "        target_song_size = self.decoder.fc2.out_features\n",
    "        \n",
    "        outputs = torch.zeros(batch_size, target_len, target_song_size) # torch.Size([64, 5, 716557])\n",
    "        decoder_input = torch.zeros(batch_size, 1).long().to(hyperparams.get_device())  # SOS token as the first input, torch.Size([64, 1])\n",
    "        for t in range(0, target_len):\n",
    "            decoder_output, hidden = self.decoder(decoder_input.unsqueeze(1), hidden, encoder_output)\n",
    "            outputs[:, t, :] = decoder_output\n",
    "            # using teacher forcing\n",
    "            if use_teacher_forcing:\n",
    "                assert target_song_ids is not None, \"Teacher forcing must have a target; it shouldn't be None.\"\n",
    "                decoder_input = target_song_ids[:, t].unsqueeze(1)\n",
    "            else:\n",
    "                decoder_input = torch.argmax(decoder_output.squeeze(0),dim = 1).unsqueeze(1)\n",
    "\n",
    "        return torch.argmax(outputs, dim = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(hyperparams.get_hidden_size(), \n",
    "                  train_dataset[0]['feature'].shape[0], \n",
    "                  hyperparams.get_lstm_num_layers()\n",
    "                  )\n",
    "decoder = Decoder(len(ID_to_IDX), \n",
    "                  hyperparams.get_embedding_dim(), \n",
    "                  hyperparams.get_hidden_size(), \n",
    "                  hyperparams.get_hidden_size(), \n",
    "                  hyperparams.get_lstm_num_layers()\n",
    "                  )\n",
    "model = Seq2Seq(encoder, decoder).to(hyperparams.get_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_model(model=model, train_dataloader=train_dataloader, inference=False):\n",
    "    # last batch\n",
    "    batch_sample = None\n",
    "    for idx, data in enumerate(train_dataloader):\n",
    "        if idx == len(train_dataloader)-1:\n",
    "            batch_sample = data\n",
    "    #batch_sample = next(iter(train_dataloader))\n",
    "    features = batch_sample['feature'].to(hyperparams.get_device())\n",
    "    print(\"features: \", features.shape)\n",
    "    target   = batch_sample['label'].to(hyperparams.get_device())\n",
    "    print(\"target: \", target.shape)\n",
    "    # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        if inference:\n",
    "            output = model(features, None, False)\n",
    "        else:\n",
    "            output = model(features, target)\n",
    "        return output.shape, output\n",
    "testing_model(inference=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss function, Optimizer, Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class listNetLoss(torch.nn.Module):\n",
    "    def __init__(self, eps=1e-10, padded_value_indicator=-1):\n",
    "        super(listNetLoss, self).__init__()\n",
    "        self.eps = eps\n",
    "        self.padded_value_indicator = padded_value_indicator\n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "    def forward(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        ListNet loss introduced in \"Learning to Rank: From Pairwise Approach to Listwise Approach\".\n",
    "        :param y_pred: predictions from the model, shape [batch_size, slate_length]\n",
    "        :param y_true: ground truth labels, shape [batch_size, slate_length]\n",
    "        :param eps: epsilon value, used for numerical stability\n",
    "        :param padded_value_indicator: an indicator of the y_true index containing a padded item, e.g. -1\n",
    "        :return: loss value, a torch.Tensor\n",
    "        \"\"\"\n",
    "        y_pred = y_pred.float()\n",
    "        y_true = y_true.float()\n",
    "        y_pred = y_pred.detach().requires_grad_(True)\n",
    "        y_pred,_ = torch.sort(y_pred, dim=1)\n",
    "        y_true,_ = torch.sort(y_true, dim=1)\n",
    "        preds_smax = self.softmax(y_pred) + self.eps\n",
    "        true_smax  = self.softmax(y_true)\n",
    "        preds_smax = preds_smax \n",
    "        preds_log = torch.log(preds_smax)\n",
    "        return torch.mean(-torch.sum(true_smax * preds_log, dim=1))\n",
    "    \n",
    "def test_Loss(loss_fn=listNetLoss()):\n",
    "    # Example usage:\n",
    "    predictions = torch.tensor(\n",
    "        [[     0, 627674, 217020, 131695, 131695, 131695],\n",
    "         [     0,  43503, 502994, 472149, 639739, 585053],\n",
    "         [     0,  43503, 169674, 169674, 217020, 585053],\n",
    "         [     0, 696212, 231735, 231735, 272798, 272798],\n",
    "         [     0,  43503, 512256, 512256, 667592, 137733],\n",
    "         [     0,  43503, 231735, 169674, 169674, 169674],\n",
    "         [     0,  43503, 144857, 667592, 667592, 137733],\n",
    "         [     0,  43503, 217020, 585053, 667592, 585053],\n",
    "         [     0,  43503, 231735, 217020, 667592, 634661],\n",
    "         [     0,  43503, 144857, 355345, 137733, 137733],\n",
    "         [     0,  43503, 169674, 169674, 169674, 169674],\n",
    "         [     0,  43503, 231735, 512256, 639739, 169674],\n",
    "         [     0, 696212, 231735, 231735, 231735, 231232],\n",
    "         [     0,  43503, 231735, 217020, 169674, 639739],\n",
    "         [     0,  43503, 562567, 169674, 634661, 634661],\n",
    "         [     0,  43503, 217020, 667592, 667592, 634661]]).float() # Example predicted scores\n",
    "    labels = torch.tensor(\n",
    "        [[     0, 627674, 217020, 131695, 131695, 131695],\n",
    "         [     0,  43503, 502994, 472149, 639739, 585053],\n",
    "         [     0,  43503, 169674, 169674, 217020, 585053],\n",
    "         [     0, 696212, 231735, 231735, 272798, 272798],\n",
    "         [     0,  43503, 512256, 512256, 667592, 137733],\n",
    "         [     0,  43503, 2, 3, 169674, 169674],\n",
    "         [     0,  43503, 144857, 667592, 667592, 137733],\n",
    "         [     0,  43503, 217020, 585053, 667592, 585053],\n",
    "         [     0,  43503, 231735, 217020, 667592, 634661],\n",
    "         [     0,  43503, 144857, 355345, 137733, 137733],\n",
    "         [     0,  43503, 169674, 169674, 5, 169674],\n",
    "         [     0,  43503, 231735, 512256, 639739, 169674],\n",
    "         [     0, 696212, 231735, 4, 231735, 231232],\n",
    "         [     0,  43503, 231735, 217020, 169674, 639739],\n",
    "         [     0,  43503, 562567, 169674, 634661, 634661],\n",
    "         [     0,  43503, 217020, 667592, 667592, 634661]]).float() # Example true relevance scores \n",
    "    # Define nDCG loss criterion\n",
    "    criterion = loss_fn\n",
    "    # Calculate nDCG loss\n",
    "    loss = criterion(predictions[:,1:].detach().requires_grad_(True), labels[:,1:])\n",
    "    loss.backward()\n",
    "    print(f\"Loss: {loss.item()}\")\n",
    "    \n",
    "test_Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = listNetLoss()\n",
    "# loss_fn = nn.MSELoss()\n",
    "# loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=hyperparams.get_learning_rate())\n",
    "total_steps = len(train_dataloader) * hyperparams.get_epochs()\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_loader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch in tqdm(data_loader):\n",
    "        optimizer.zero_grad()\n",
    "        features = batch['feature'].to(device)\n",
    "        target   = batch['label'].squeeze(1).to(device)\n",
    "        outputs = model(features, target)\n",
    "        loss = loss_fn(outputs[:,1:].cpu(), target[:,1:].cpu())\n",
    "        total_loss+=loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    return total_loss\n",
    "\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader):\n",
    "            features = batch['feature'].to(device)\n",
    "            target   = batch['label'].squeeze(1).to(device)\n",
    "            outputs = model(features, target)\n",
    "            loss = loss_fn(outputs[:,1:].cpu(), target[:,1:].cpu())\n",
    "            total_loss+=loss.item()\n",
    "    return total_loss\n",
    "\n",
    "def training_Process():\n",
    "    train_losses, valid_losses = [], []\n",
    "    print(\"\\nStart Training:\")\n",
    "    for epoch in range(hyperparams.get_epochs()):\n",
    "        print(f\"Epoch {epoch + 1}/{hyperparams.get_epochs()}\")\n",
    "        train_loss = train(model, train_dataloader, optimizer, scheduler, hyperparams.get_device())\n",
    "        valid_loss = evaluate(model, val_dataloader, hyperparams.get_device())\n",
    "        train_losses.append(train_loss)\n",
    "        valid_losses.append(valid_loss)\n",
    "        print(f\"Training loss   : {train_loss:.4f}\")\n",
    "        print(f\"Validation loss : {valid_loss:.4f}\")\n",
    "    return train_losses, valid_losses\n",
    "\n",
    "train_losses, valid_losses = training_Process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_and_loss(model, folder_path, valid_losses):\n",
    "    # Create the folder if it doesn't exist\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    # Save the model state dict\n",
    "    mean_valid_loss = sum(valid_losses) / len(valid_losses)\n",
    "    torch.save(model.state_dict(), os.path.join(folder_path, f'model_loss_{mean_valid_loss:.4f}.pth'))\n",
    "\n",
    "def load_model(model, model_path):\n",
    "    # Load the model state dict\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    # Set the model to evaluation mode after loading\n",
    "    model.eval()  \n",
    "    return model\n",
    "\n",
    "# save model\n",
    "save_model_and_loss(model, hyperparams.get_save_dir(), valid_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Plotting\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(valid_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Losses')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Submittion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prediction(model, test_dataloader, device, batch=False):\n",
    "    if batch:\n",
    "        batch = next(iter(test_dataloader))\n",
    "        session_id = batch['session_id']\n",
    "        features = batch['feature'].to(device)\n",
    "        outputs = model(features, target_song_ids=None, use_teacher_forcing=False)\n",
    "        return session_id, outputs[:,1:]\n",
    "    else:\n",
    "        total_session_ids = torch.zeros(len(test_dataloader)*hyperparams.get_batch_size())\n",
    "        total_outputs = torch.zeros(len(test_dataloader)*hyperparams.get_batch_size(), 5)\n",
    "        # batch = next(iter(test_dataloader))\n",
    "        for idx, batch in tqdm(enumerate(test_dataloader)):\n",
    "            session_id = batch['session_id']\n",
    "            features = batch['feature'].to(device)\n",
    "            outputs = model(features, target_song_ids=None, use_teacher_forcing=False, prediction=True)\n",
    "            total_session_ids[hyperparams.get_batch_size()*idx:hyperparams.get_batch_size()*(idx+1)] = session_id\n",
    "            total_outputs[hyperparams.get_batch_size()*idx:hyperparams.get_batch_size()*(idx+1), :] = outputs[:,1:]\n",
    "        return total_session_ids, total_outputs\n",
    "\n",
    "total_session_ids, total_outputs = generate_prediction(model, test_dataloader, hyperparams.get_device(), batch=True)\n",
    "\n",
    "final_submittion_df = pd.DataFrame({\n",
    "    \"session_id\":total_session_ids.numpy(),\n",
    "    \"top1\":total_outputs[:,0].numpy(),\n",
    "    \"top2\":total_outputs[:,1].numpy(),\n",
    "    \"top3\":total_outputs[:,2].numpy(),\n",
    "    \"top4\":total_outputs[:,3].numpy(),\n",
    "    \"top5\":total_outputs[:,4].numpy()\n",
    "})\n",
    "final_submittion_df['top1'] = final_submittion_df['top1'].map(IDX_to_ID)\n",
    "final_submittion_df['top2'] = final_submittion_df['top2'].map(IDX_to_ID)\n",
    "final_submittion_df['top3'] = final_submittion_df['top3'].map(IDX_to_ID)\n",
    "final_submittion_df['top4'] = final_submittion_df['top4'].map(IDX_to_ID)\n",
    "final_submittion_df['top5'] = final_submittion_df['top5'].map(IDX_to_ID)\n",
    "# save the final submission\n",
    "# Get today's date\n",
    "current_time = datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "save_folder = \"../submission/\"\n",
    "file_name = f'{current_time}.csv'\n",
    "final_submittion_df.to_csv(save_folder+file_name, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
