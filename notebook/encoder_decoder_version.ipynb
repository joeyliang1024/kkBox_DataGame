{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-21 22:47:43.875882: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-21 22:47:43.875930: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-21 22:47:43.877295: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import warnings\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.SettingWithCopyWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper-Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperParameters:\n",
    "    def __init__(self, batch_size=512, \n",
    "                 learning_rate=0.0001, \n",
    "                 epochs=10, # 1 for testing\n",
    "                 hidden_size = 256,\n",
    "                 embedding_dim = 128,\n",
    "                 lstm_num_layers = 3,\n",
    "                 train_size = 0.8,\n",
    "                 device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "                 save_dir = \"../model/encoder-decoder/\"\n",
    "                 ):\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.lstm_num_layers = lstm_num_layers\n",
    "        self.train_size = train_size\n",
    "        self.device = device\n",
    "        self.save_dir = save_dir\n",
    "\n",
    "    def get_batch_size(self):\n",
    "        return self.batch_size\n",
    "\n",
    "    def get_learning_rate(self):\n",
    "        return self.learning_rate\n",
    "\n",
    "    def get_epochs(self):\n",
    "        return self.epochs\n",
    "\n",
    "    def get_hidden_size(self):\n",
    "        return self.hidden_size\n",
    "    \n",
    "    def get_embedding_dim(self):\n",
    "        return self.embedding_dim\n",
    "    \n",
    "    def get_lstm_num_layers(self):\n",
    "        return self.lstm_num_layers\n",
    "    \n",
    "    def get_train_size(self):\n",
    "        return self.train_size\n",
    "    \n",
    "    def get_device(self):\n",
    "        return self.device\n",
    "    \n",
    "    def get_save_dir(self):\n",
    "        return self.save_dir\n",
    "    \n",
    "hyperparams = HyperParameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values (NaNs) in the dataset: meta_song.parquet\n",
      "song_id             0\n",
      "artist_id      128866\n",
      "song_length    128866\n",
      "album_id       323497\n",
      "language_id    323497\n",
      "album_month    323523\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Read the Parquet file and drop duplicates based on \"song_id\" column\n",
    "df = pd.read_parquet(\"../data/meta_song.parquet\").drop_duplicates(\"song_id\")\n",
    "# Count the number of missing values (NaNs) in the DataFrame\n",
    "na_count = df.isna().sum()\n",
    "print(\"Number of missing values (NaNs) in the dataset: meta_song.parquet\")\n",
    "print(na_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_feacture_dataset(df:pd.DataFrame)->pd.DataFrame:\n",
    "    shape = df.shape[0]\n",
    "    df1 = pd.read_parquet(\"../data/meta_song_composer.parquet\").drop_duplicates(\"song_id\")\n",
    "    print(len(set(df['song_id'].unique())-set(df1['song_id'].unique())))\n",
    "    df = pd.merge(df, df1, how='left') \n",
    "    assert df.shape[0] == shape, f\"origin shape: {shape}, merge after shape: {df.shape[0]}\"\n",
    "    df2 = pd.read_parquet(\"../data/meta_song_genre.parquet\").drop_duplicates(\"song_id\")\n",
    "    print(len(set(df['song_id'].unique())-set(df2['song_id'].unique())))\n",
    "    df = pd.merge(df, df2, how='left') \n",
    "    assert df.shape[0] == shape, f\"origin shape: {shape}, merge after shape: {df.shape[0]}\"\n",
    "    df3 = pd.read_parquet(\"../data/meta_song_lyricist.parquet\").drop_duplicates(\"song_id\")\n",
    "    print(len(set(df['song_id'].unique())-set(df3['song_id'].unique())))\n",
    "    df = pd.merge(df, df3, how='left') \n",
    "    assert df.shape[0] == shape, f\"origin shape: {shape}, merge after shape: {df.shape[0]}\"\n",
    "    df4 = pd.read_parquet(\"../data/meta_song_producer.parquet\").drop_duplicates(\"song_id\")\n",
    "    print(len(set(df['song_id'].unique())-set(df4['song_id'].unique())))\n",
    "    df = pd.merge(df, df4, how='left') \n",
    "    assert df.shape[0] == shape, f\"origin shape: {shape}, merge after shape: {df.shape[0]}\"\n",
    "    df5 = pd.read_parquet(\"../data/meta_song_titletext.parquet\").drop_duplicates(\"song_id\")\n",
    "    print(len(set(df['song_id'].unique())-set(df5['song_id'].unique())))\n",
    "    df = pd.merge(df, df5, how='left') \n",
    "    assert df.shape[0] == shape, f\"origin shape: {shape}, merge after shape: {df.shape[0]}\"\n",
    "    df6 = pd.read_parquet(\"../data/meta_song.parquet\").drop_duplicates(\"song_id\")\n",
    "    print(len(set(df['song_id'].unique())-set(df6['song_id'].unique())))\n",
    "    df = pd.merge(df, df6, how='left') \n",
    "    assert df.shape[0] == shape, f\"origin shape: {shape}, merge after shape: {df.shape[0]}\"\n",
    "    print(f\"Merge finish!, now shape is : {df.shape}\")\n",
    "    return df\n",
    "\n",
    "# load from directory\n",
    "train_source = pd.read_parquet(\"../data/label_train_source.parquet\")\n",
    "train_target = pd.read_parquet(\"../data/label_train_target.parquet\")\n",
    "test_source  = pd.read_parquet(\"../data/label_test_source.parquet\")\n",
    "# sort the data by session_id, listening_order\n",
    "train_source = train_source.sort_values(by=['session_id', 'listening_order'], ascending=[True, True])\n",
    "train_target = train_target.sort_values(by=['session_id', 'listening_order'], ascending=[True, True])\n",
    "test_source  = test_source.sort_values( by=['session_id', 'listening_order'], ascending=[True, True])\n",
    "\n",
    "#train_source = merge_feacture_dataset(train_source)\n",
    "#train_target = merge_feacture_dataset(train_target)\n",
    "#test_source  = merge_feacture_dataset(test_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode the feactures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_NaNs(df:pd.DataFrame, numerical_columns:list=None, string_columns:list=None)->pd.DataFrame:\n",
    "    for column in numerical_columns:\n",
    "        df[column].fillna(df[column].mean(), inplace=True)\n",
    "    for column in string_columns:\n",
    "        df[column].fillna(0)\n",
    "    return df\n",
    "\n",
    "def encode_unix_time(df:pd.DataFrame, sin_cos = False):\n",
    "    # Convert 'unix_played_at' to a datetime column\n",
    "    df['played_at_datetime'] = pd.to_datetime(df['unix_played_at'], unit='s')\n",
    "    if sin_cos:\n",
    "        df['hour_sin'] = np.sin(2 * np.pi * df['played_at_datetime'].dt.hour / 24)\n",
    "        df['hour_cos'] = np.cos(2 * np.pi * df['played_at_datetime'].dt.hour / 24)\n",
    "        df['minute_sin'] = np.sin(2 * np.pi * df['played_at_datetime'].dt.minute / 60)\n",
    "        df['minute_cos'] = np.cos(2 * np.pi * df['played_at_datetime'].dt.minute / 60)\n",
    "        df['second_sin'] = np.sin(2 * np.pi * df['played_at_datetime'].dt.second / 60)\n",
    "        df['second_cos'] = np.cos(2 * np.pi * df['played_at_datetime'].dt.second / 60)\n",
    "        df['month_sin'] = np.sin(2 * np.pi * df['played_at_datetime'].dt.month / 12)\n",
    "        df['month_cos'] = np.cos(2 * np.pi * df['played_at_datetime'].dt.month / 12)\n",
    "        df['year_sin'] = np.sin(2 * np.pi * df['played_at_datetime'].dt.year / 2023)\n",
    "        df['year_cos'] = np.cos(2 * np.pi * df['played_at_datetime'].dt.year / 2023)\n",
    "    else:\n",
    "        df['hour_of_day'] = df['played_at_datetime'].dt.hour / 24\n",
    "        df['minute_of_hour'] = df['played_at_datetime'].dt.minute / 60\n",
    "        df['second_of_minute'] = df['played_at_datetime'].dt.second / 60\n",
    "        df['month'] = df['played_at_datetime'].dt.month / 12\n",
    "        df['year'] = df['played_at_datetime'].dt.year / 2023\n",
    "    # Drop the specified columns from the DataFrame\n",
    "    df.drop(columns=['unix_played_at', 'played_at_datetime'], inplace=True)\n",
    "    return df\n",
    "\n",
    "def get_song_ID_encode_dict(train:pd.DataFrame, test:pd.DataFrame)->dict:\n",
    "    unique_song_ids = set(train['song_id'].tolist()+test['song_id'].tolist())\n",
    "    ID_IDX = {song_id:i+1 for i,song_id in enumerate(unique_song_ids)}\n",
    "    ID_IDX[\"SOS\"]=0\n",
    "    return ID_IDX\n",
    "\n",
    "def encode_song_id(id2idx:dict, source_df:pd.DataFrame, target_df:pd.DataFrame=None):\n",
    "    source_df['song_id'] = source_df['song_id'].map(id2idx)\n",
    "    if target_df is not None:\n",
    "        target_df['song_id'] = target_df['song_id'].map(id2idx)\n",
    "        return source_df, target_df\n",
    "    else:\n",
    "        return source_df, None\n",
    "\n",
    "def convert_per_N(df:pd.DataFrame, n:int, label = False):\n",
    "    data = []\n",
    "    pre_session_id = int(df['session_id'].iloc[0])\n",
    "    if label:\n",
    "        row = [0]\n",
    "        for i in tqdm(range(df.shape[0])):\n",
    "            # next session id\n",
    "            if pre_session_id != int(df['session_id'].iloc[i]):\n",
    "                data.append((pre_session_id, np.array(row).reshape(-1, 6)))\n",
    "                pre_session_id, row = int(df['session_id'].iloc[i]), [0]\n",
    "            # append 5 values\n",
    "            row.append(df['song_id'].iloc[i])\n",
    "        # append last session id\n",
    "        data.append((df['session_id'].iloc[-1], np.array(row).reshape(-1, 6))) #last one\n",
    "    else:\n",
    "        row = []\n",
    "        song_id = []\n",
    "        for i in tqdm(range(df.shape[0])):\n",
    "            # next session id\n",
    "            if pre_session_id != int(df['session_id'].iloc[i]):\n",
    "                data.append((pre_session_id, \n",
    "                             np.array(row).reshape(-1, 20), \n",
    "                             np.array(song_id).reshape(-1, 20)))\n",
    "                pre_session_id, row,song_id = int(df['session_id'].iloc[i]), [], []\n",
    "            # append 20 values\n",
    "            song_id.append(df['song_id'].iloc[i])\n",
    "            row.append([df['play_status'].iloc[i],\n",
    "                        df['login_type'].iloc[i],\n",
    "                        df['second_of_minute'].iloc[i],\n",
    "                        df['minute_of_hour'].iloc[i],\n",
    "                        df['hour_of_day'].iloc[i],\n",
    "                        df['month'].iloc[i], \n",
    "                        df['year'].iloc[i]])\n",
    "        # append last session id\n",
    "        data.append((df['session_id'].iloc[-1], \n",
    "                     np.array(row).reshape(-1, 20),\n",
    "                     np.array(song_id).reshape(-1, 20)))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11445180/11445180 [08:44<00:00, 21837.33it/s]\n",
      "100%|██████████| 2861295/2861295 [00:32<00:00, 87361.13it/s]\n",
      "100%|██████████| 2861280/2861280 [02:10<00:00, 21857.78it/s]\n"
     ]
    }
   ],
   "source": [
    "# data preprocessing\n",
    "ID_IDX = get_song_ID_encode_dict(train_source, test_source)\n",
    "# encode song_id\n",
    "train_source, train_target = encode_song_id(ID_IDX, train_source, train_target)\n",
    "test_source, _ =  encode_song_id(ID_IDX, test_source, None)\n",
    "# encode unix_time_at\n",
    "train_source = encode_unix_time(train_source)\n",
    "train_target = encode_unix_time(train_target)\n",
    "test_source  = encode_unix_time(test_source)\n",
    "# covert to sequential data, top 20 songs for input, last 5 for output\n",
    "train_source_data  = convert_per_N(train_source, 20)\n",
    "train_source_label = convert_per_N(train_target, 5, label=True)\n",
    "test_source_data   = convert_per_N(test_source, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to Dataset and Dataloader with batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RankingDatset(Dataset):\n",
    "    def __init__(self, data, label=None, train = True):\n",
    "        self.session_id = [session_id for session_id,_,_ in data]\n",
    "        self.feature    = [feature    for _,feature,_    in data]\n",
    "        self.song_id    = [song_id    for _,_,song_id    in data]\n",
    "        if train:\n",
    "            self.label  = [label for _,label in label]\n",
    "        else:\n",
    "            self.label  = [0 for _ in data]\n",
    "    def __len__(self):\n",
    "        return len(self.session_id)\n",
    "    def __getitem__(self, idx):\n",
    "        session_id = self.session_id[idx]\n",
    "        feature = torch.tensor(self.feature[idx], dtype=torch.long)\n",
    "        song_id = torch.tensor(self.song_id[idx], dtype=torch.long)\n",
    "        label = torch.tensor(self.label[idx], dtype=torch.long)\n",
    "        return {'session_id': session_id, 'feature': feature, 'song_id': song_id, 'label': label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'session_id': 96605,\n",
       " 'feature': tensor([[ 1, 49,  0,  0,  0,  0,  0,  1, 49,  0,  0,  0,  0,  0,  1, 49,  0,  0,\n",
       "           0,  0],\n",
       "         [ 0,  1, 49,  0,  0,  0,  0,  0,  1, 49,  0,  0,  0,  0,  0,  1, 49,  0,\n",
       "           0,  0],\n",
       "         [ 0,  0,  1, 49,  0,  0,  0,  0,  0,  1, 49,  0,  0,  0,  0,  0,  1, 49,\n",
       "           0,  0],\n",
       "         [ 0,  0,  0,  1, 49,  0,  0,  0,  0,  0,  1, 49,  0,  0,  0,  0,  0,  1,\n",
       "          49,  0],\n",
       "         [ 0,  0,  0,  0,  1, 49,  0,  0,  0,  0,  0,  1, 49,  0,  0,  0,  0,  0,\n",
       "           1, 49],\n",
       "         [ 0,  0,  0,  0,  0,  1, 49,  0,  0,  0,  0,  0,  1, 49,  0,  0,  0,  0,\n",
       "           0,  1],\n",
       "         [49,  0,  0,  0,  0,  0,  1, 49,  0,  0,  0,  0,  0,  1, 49,  0,  0,  0,\n",
       "           0,  0]]),\n",
       " 'song_id': tensor([[ 82580, 606893, 208757, 527901, 232236,  62746, 177114, 211785, 481046,\n",
       "          213249, 624855, 222640, 168755, 292540, 592736,  74856, 221164, 298480,\n",
       "          335012, 277550]]),\n",
       " 'label': tensor([[     0, 572953, 239671, 477962, 685485,  87351]])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = RankingDatset(train_source_data, train_source_label)\n",
    "train_size = int(hyperparams.get_train_size() * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
    "test_dataset  = RankingDatset(test_source_data, train=False)\n",
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=hyperparams.get_batch_size(), shuffle=True)\n",
    "val_dataloader   = DataLoader(val_dataset, batch_size=hyperparams.get_batch_size())\n",
    "test_dataloader  = DataLoader(test_dataset, batch_size=hyperparams.get_batch_size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_songs, embedding_dim, hidden_size, num_feature, num_layers=1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(num_songs, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim+num_feature, hidden_size, num_layers, batch_first=True)\n",
    "\n",
    "    def forward(self, song_ids, feature):\n",
    "        # song_ids: torch.Size([batch size, 1, 20])\n",
    "        # feature : torch.Size([batch size, 7, 20])\n",
    "        embedded = self.embedding(song_ids) .squeeze(1)\n",
    "        # input feature cat with song embed\n",
    "        lstm_input = torch.cat((embedded, feature.transpose(1,2)), dim=2)  # Concatenate along the feature dimension\n",
    "        # Forward propagate LSTM\n",
    "        out, hidden = self.lstm(lstm_input)  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        return out, hidden\n",
    "\n",
    "# Define the Decoder\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_songs, embedding_dim, enc_hidden_size, hidden_size, num_layers=1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_songs, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim+enc_hidden_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc1 = nn.Linear(20, 1)\n",
    "        self.fc2 = nn.Linear(hidden_size, num_songs)\n",
    "\n",
    "    def forward(self, decode_song_ids, last_hidden, encoder_hidden):\n",
    "        # Forward propagate LSTM\n",
    "        embedded = self.embedding(decode_song_ids).squeeze(1) # torch.Size([64, 1, 128])\n",
    "        # encoder_hidden: torch.Size([batch size, 20, 256])\n",
    "        encoder_hidden = self.fc1(encoder_hidden.transpose(1,2)) # torch.Size([64, 256, 1])\n",
    "        lstm_input = torch.cat((embedded, encoder_hidden.transpose(1,2)), dim=2) # torch.Size([64, 1, 386])\n",
    "        out, lstm_hidden = self.lstm(lstm_input, last_hidden)\n",
    "        out = self.fc2(out.squeeze(1)) # torch.Size([64, 716557])\n",
    "        return out, lstm_hidden\n",
    "\n",
    "# Seq2Seq model\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, song_ids, features, target_song_ids=None, use_teacher_forcing=True):\n",
    "        encoder_output, hidden = self.encoder(song_ids, features)\n",
    "        batch_size = song_ids.shape[0]  # 64\n",
    "        target_song_ids = torch.zeros(batch_size, 6).long().to(hyperparams.get_device()) if target_song_ids is None else target_song_ids.squeeze(1)\n",
    "        target_len = target_song_ids.size(-1) # 6\n",
    "        target_song_size = self.decoder.fc2.out_features\n",
    "        \n",
    "        outputs = torch.zeros(batch_size, target_len, target_song_size) # torch.Size([64, 6, 716557])\n",
    "        decoder_input = target_song_ids[:, 0].unsqueeze(1)  # SOS token as the first input, torch.Size([64, 1])\n",
    "\n",
    "        for t in range(1, target_len):\n",
    "            decoder_output, hidden = self.decoder(decoder_input.unsqueeze(1), hidden, encoder_output)\n",
    "            outputs[:, t, :] = decoder_output\n",
    "            # using teacher forcing\n",
    "            if use_teacher_forcing:\n",
    "                assert target_song_ids is not None, \"Teacher forcing must have a target; it shouldn't be None.\"\n",
    "                decoder_input = target_song_ids[:, t].unsqueeze(1)\n",
    "            else:\n",
    "                decoder_input = torch.argmax(decoder_output.squeeze(0),dim = 1).unsqueeze(1)\n",
    "\n",
    "        return torch.argmax(outputs, dim = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(len(list(ID_IDX.values())), \n",
    "                  hyperparams.get_embedding_dim(), \n",
    "                  hyperparams.get_hidden_size(), \n",
    "                  train_dataset[0]['feature'].shape[0], \n",
    "                  hyperparams.get_lstm_num_layers()\n",
    "                  )\n",
    "decoder = Decoder(len(list(ID_IDX.values())), \n",
    "                  hyperparams.get_embedding_dim(), \n",
    "                  hyperparams.get_hidden_size(), \n",
    "                  hyperparams.get_hidden_size(), \n",
    "                  hyperparams.get_lstm_num_layers()\n",
    "                  )\n",
    "model = Seq2Seq(encoder, decoder).to(hyperparams.get_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([79, 1, 20])\n",
      "torch.Size([79, 7, 20])\n",
      "torch.Size([79, 1, 6])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([79, 6])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def testing_model(model=model, train_dataloader=train_dataloader, inference=False):\n",
    "    # last batch\n",
    "    batch_sample = None\n",
    "    for idx, data in enumerate(train_dataloader):\n",
    "        if idx == len(train_dataloader)-1:\n",
    "            batch_sample = data\n",
    "    #batch_sample = next(iter(train_dataloader))\n",
    "    song_ids = batch_sample['song_id'].to(hyperparams.get_device())\n",
    "    print(\"song_ids: \", song_ids.shape)\n",
    "    features = batch_sample['feature'].to(hyperparams.get_device())\n",
    "    print(\"features: \", features.shape)\n",
    "    target   = batch_sample['label'].to(hyperparams.get_device())\n",
    "    print(\"target: \", target.shape)\n",
    "    # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        if inference:\n",
    "            output = model(song_ids, features, None, False)\n",
    "        else:\n",
    "            output = model(song_ids, features, target)\n",
    "        return output.shape #, output\n",
    "testing_model(inference=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss function, Optimizer, Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.9361662864685059\n"
     ]
    }
   ],
   "source": [
    "class NDCG_Score(torch.nn.Module):\n",
    "    def __init__(self, sigma=1.0):\n",
    "        super(NDCG_Score, self).__init__()\n",
    "        self.sigma = sigma\n",
    "    def forward(self, predictions, labels):\n",
    "        # Calculate nDCG loss\n",
    "        # Compare predictions and labels element-wise\n",
    "        gain = (predictions != labels).float()\n",
    "        weightage = torch.tensor([1.0, 0.63, 0.5, 0.43, 0.38], requires_grad=True).float()\n",
    "        return sum(gain @ weightage)\n",
    "\n",
    "class listNetLoss(torch.nn.Module):\n",
    "    def __init__(self, eps=1e-10, padded_value_indicator=-1):\n",
    "        super(listNetLoss, self).__init__()\n",
    "        self.eps = eps\n",
    "        self.padded_value_indicator = padded_value_indicator\n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "    def forward(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        ListNet loss introduced in \"Learning to Rank: From Pairwise Approach to Listwise Approach\".\n",
    "        :param y_pred: predictions from the model, shape [batch_size, slate_length]\n",
    "        :param y_true: ground truth labels, shape [batch_size, slate_length]\n",
    "        :param eps: epsilon value, used for numerical stability\n",
    "        :param padded_value_indicator: an indicator of the y_true index containing a padded item, e.g. -1\n",
    "        :return: loss value, a torch.Tensor\n",
    "        \"\"\"\n",
    "        y_pred = y_pred.float()\n",
    "        y_true = y_true.float()\n",
    "        y_pred = y_pred.detach().requires_grad_(True)\n",
    "        y_pred,_ = torch.sort(y_pred, dim=1)\n",
    "        y_true,_ = torch.sort(y_true, dim=1)\n",
    "        preds_smax = self.softmax(y_pred) + self.eps\n",
    "        true_smax  = self.softmax(y_true)\n",
    "        preds_smax = preds_smax \n",
    "        preds_log = torch.log(preds_smax)\n",
    "        return torch.mean(-torch.sum(true_smax * preds_log, dim=1))\n",
    "\n",
    "def test_Loss(loss_fn=listNetLoss()):\n",
    "    # Example usage:\n",
    "    predictions = torch.tensor(\n",
    "        [[     0, 627674, 217020, 131695, 131695, 131695],\n",
    "         [     0,  43503, 502994, 472149, 639739, 585053],\n",
    "         [     0,  43503, 169674, 169674, 217020, 585053],\n",
    "         [     0, 696212, 231735, 231735, 272798, 272798],\n",
    "         [     0,  43503, 512256, 512256, 667592, 137733],\n",
    "         [     0,  43503, 231735, 169674, 169674, 169674],\n",
    "         [     0,  43503, 144857, 667592, 667592, 137733],\n",
    "         [     0,  43503, 217020, 585053, 667592, 585053],\n",
    "         [     0,  43503, 231735, 217020, 667592, 634661],\n",
    "         [     0,  43503, 144857, 355345, 137733, 137733],\n",
    "         [     0,  43503, 169674, 169674, 169674, 169674],\n",
    "         [     0,  43503, 231735, 512256, 639739, 169674],\n",
    "         [     0, 696212, 231735, 231735, 231735, 231232],\n",
    "         [     0,  43503, 231735, 217020, 169674, 639739],\n",
    "         [     0,  43503, 562567, 169674, 634661, 634661],\n",
    "         [     0,  43503, 217020, 667592, 667592, 634661]]).float() # Example predicted scores\n",
    "    labels = torch.tensor(\n",
    "        [[     0, 627674, 217020, 131695, 131695, 131695],\n",
    "         [     0,  43503, 502994, 472149, 639739, 585053],\n",
    "         [     0,  43503, 169674, 169674, 217020, 585053],\n",
    "         [     0, 696212, 231735, 231735, 272798, 272798],\n",
    "         [     0,  43503, 512256, 512256, 667592, 137733],\n",
    "         [     0,  43503, 2, 3, 169674, 169674],\n",
    "         [     0,  43503, 144857, 667592, 667592, 137733],\n",
    "         [     0,  43503, 217020, 585053, 667592, 585053],\n",
    "         [     0,  43503, 231735, 217020, 667592, 634661],\n",
    "         [     0,  43503, 144857, 355345, 137733, 137733],\n",
    "         [     0,  43503, 169674, 169674, 5, 169674],\n",
    "         [     0,  43503, 231735, 512256, 639739, 169674],\n",
    "         [     0, 696212, 231735, 4, 231735, 231232],\n",
    "         [     0,  43503, 231735, 217020, 169674, 639739],\n",
    "         [     0,  43503, 562567, 169674, 634661, 634661],\n",
    "         [     0,  43503, 217020, 667592, 667592, 634661]]).float() # Example true relevance scores \n",
    "    # Define nDCG loss criterion\n",
    "    criterion = loss_fn\n",
    "    # Calculate nDCG loss\n",
    "    loss = criterion(predictions[:,1:].detach().requires_grad_(True), labels[:,1:])\n",
    "    loss.backward()\n",
    "    print(f\"Loss: {loss.item()}\")\n",
    "    \n",
    "test_Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = listNetLoss()\n",
    "# loss_fn = nn.MSELoss()\n",
    "# loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=hyperparams.get_learning_rate())\n",
    "total_steps = len(train_dataloader) * hyperparams.get_epochs()\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start Training:\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 894/895 [1:19:00<00:05,  5.30s/it]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[512, 20, -1]' is invalid for input of size 11060",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/work/u5110390/kkBox_game/notebook/kkbox_preprocessing.ipynb 儲存格 23\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2254574343285079746f72636829227d/work/u5110390/kkBox_game/notebook/kkbox_preprocessing.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=48'>49</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mValidation loss : \u001b[39m\u001b[39m{\u001b[39;00mvalid_loss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2254574343285079746f72636829227d/work/u5110390/kkBox_game/notebook/kkbox_preprocessing.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=49'>50</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m train_losses, valid_losses\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2254574343285079746f72636829227d/work/u5110390/kkBox_game/notebook/kkbox_preprocessing.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=51'>52</a>\u001b[0m train_losses, valid_losses \u001b[39m=\u001b[39m training_Process()\n",
      "\u001b[1;32m/work/u5110390/kkBox_game/notebook/kkbox_preprocessing.ipynb 儲存格 23\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2254574343285079746f72636829227d/work/u5110390/kkBox_game/notebook/kkbox_preprocessing.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(hyperparams\u001b[39m.\u001b[39mget_epochs()):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2254574343285079746f72636829227d/work/u5110390/kkBox_game/notebook/kkbox_preprocessing.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mhyperparams\u001b[39m.\u001b[39mget_epochs()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2254574343285079746f72636829227d/work/u5110390/kkBox_game/notebook/kkbox_preprocessing.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=43'>44</a>\u001b[0m     train_loss \u001b[39m=\u001b[39m train(model, train_dataloader, optimizer, scheduler, hyperparams\u001b[39m.\u001b[39;49mget_device(), num_class)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2254574343285079746f72636829227d/work/u5110390/kkBox_game/notebook/kkbox_preprocessing.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=44'>45</a>\u001b[0m     valid_loss \u001b[39m=\u001b[39m evaluate(model, val_dataloader, hyperparams\u001b[39m.\u001b[39mget_device(), num_class)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2254574343285079746f72636829227d/work/u5110390/kkBox_game/notebook/kkbox_preprocessing.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=45'>46</a>\u001b[0m     train_losses\u001b[39m.\u001b[39mappend(train_loss)\n",
      "\u001b[1;32m/work/u5110390/kkBox_game/notebook/kkbox_preprocessing.ipynb 儲存格 23\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2254574343285079746f72636829227d/work/u5110390/kkBox_game/notebook/kkbox_preprocessing.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m features \u001b[39m=\u001b[39m batch[\u001b[39m'\u001b[39m\u001b[39mfeature\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2254574343285079746f72636829227d/work/u5110390/kkBox_game/notebook/kkbox_preprocessing.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m target   \u001b[39m=\u001b[39m batch[\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2254574343285079746f72636829227d/work/u5110390/kkBox_game/notebook/kkbox_preprocessing.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(song_ids, features, target)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2254574343285079746f72636829227d/work/u5110390/kkBox_game/notebook/kkbox_preprocessing.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(outputs[:,\u001b[39m1\u001b[39m:]\u001b[39m.\u001b[39mcpu(), target[:,\u001b[39m1\u001b[39m:]\u001b[39m.\u001b[39mcpu())\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2254574343285079746f72636829227d/work/u5110390/kkBox_game/notebook/kkbox_preprocessing.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# loss = loss_fn(\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2254574343285079746f72636829227d/work/u5110390/kkBox_game/notebook/kkbox_preprocessing.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m#      outputs.cpu(),\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2254574343285079746f72636829227d/work/u5110390/kkBox_game/notebook/kkbox_preprocessing.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m#      torch.tensor(torch.eye(num_classes)[target.cpu()]).transpose(1,2)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2254574343285079746f72636829227d/work/u5110390/kkBox_game/notebook/kkbox_preprocessing.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m#     )\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/work/u5110390/kkBox_game/notebook/kkbox_preprocessing.ipynb 儲存格 23\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2254574343285079746f72636829227d/work/u5110390/kkBox_game/notebook/kkbox_preprocessing.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, song_ids, features, target_song_ids\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, use_teacher_forcing\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2254574343285079746f72636829227d/work/u5110390/kkBox_game/notebook/kkbox_preprocessing.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=49'>50</a>\u001b[0m     encoder_output, hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(song_ids, features)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2254574343285079746f72636829227d/work/u5110390/kkBox_game/notebook/kkbox_preprocessing.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=50'>51</a>\u001b[0m     target_song_ids \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder\u001b[39m.\u001b[39mbatch_size, \u001b[39m6\u001b[39m)\u001b[39m.\u001b[39mlong()\u001b[39m.\u001b[39mto(hyperparams\u001b[39m.\u001b[39mget_device()) \u001b[39mif\u001b[39;00m target_song_ids \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m target_song_ids\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2254574343285079746f72636829227d/work/u5110390/kkBox_game/notebook/kkbox_preprocessing.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=51'>52</a>\u001b[0m     batch_size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder\u001b[39m.\u001b[39mbatch_size  \u001b[39m# 64\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/work/u5110390/kkBox_game/notebook/kkbox_preprocessing.ipynb 儲存格 23\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2254574343285079746f72636829227d/work/u5110390/kkBox_game/notebook/kkbox_preprocessing.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m embedded \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding(song_ids) \u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2254574343285079746f72636829227d/work/u5110390/kkBox_game/notebook/kkbox_preprocessing.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# input feature cat with song embed\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2254574343285079746f72636829227d/work/u5110390/kkBox_game/notebook/kkbox_preprocessing.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m lstm_input \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((embedded, feature\u001b[39m.\u001b[39;49mview(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_size,\u001b[39m20\u001b[39;49m,\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)), dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)  \u001b[39m# Concatenate along the feature dimension\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2254574343285079746f72636829227d/work/u5110390/kkBox_game/notebook/kkbox_preprocessing.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# Forward propagate LSTM\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2254574343285079746f72636829227d/work/u5110390/kkBox_game/notebook/kkbox_preprocessing.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m out, hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlstm(lstm_input)  \u001b[39m# out: tensor of shape (batch_size, seq_length, hidden_size)\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[512, 20, -1]' is invalid for input of size 11060"
     ]
    }
   ],
   "source": [
    "def train(model, data_loader, optimizer, scheduler, device, num_classes=None):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch in tqdm(data_loader):\n",
    "        optimizer.zero_grad()\n",
    "        song_ids = batch['song_id'].to(device)\n",
    "        features = batch['feature'].to(device)\n",
    "        target   = batch['label'].squeeze(1).to(device)\n",
    "        outputs = model(song_ids, features, target)\n",
    "        loss = loss_fn(outputs[:,1:].cpu(), target[:,1:].cpu())\n",
    "        # loss = loss_fn(\n",
    "        #      outputs.cpu(),\n",
    "        #      torch.tensor(torch.eye(num_classes)[target.cpu()]).transpose(1,2)\n",
    "        #     )\n",
    "        total_loss+=loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    return total_loss\n",
    "\n",
    "def evaluate(model, data_loader, device, num_classes=None):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader):\n",
    "            song_ids = batch['song_id'].to(device)\n",
    "            features = batch['feature'].to(device)\n",
    "            target   = batch['label'].squeeze(1).to(device)\n",
    "            outputs = model(song_ids, features, target)\n",
    "            loss = loss_fn(outputs[:,1:], target[:,1:])\n",
    "            # loss = loss_fn(\n",
    "            #      outputs.cpu(),\n",
    "            #      torch.tensor(torch.eye(num_classes)[target.cpu()]).transpose(1,2)\n",
    "            # )\n",
    "            total_loss+=loss.item()\n",
    "    return total_loss\n",
    "\n",
    "def training_Process():\n",
    "    train_losses, valid_losses = [], []\n",
    "    num_class = len(ID_IDX)\n",
    "    print(\"\\nStart Training:\")\n",
    "    for epoch in range(hyperparams.get_epochs()):\n",
    "        print(f\"Epoch {epoch + 1}/{hyperparams.get_epochs()}\")\n",
    "        train_loss = train(model, train_dataloader, optimizer, scheduler, hyperparams.get_device(), num_class)\n",
    "        valid_loss = evaluate(model, val_dataloader, hyperparams.get_device(), num_class)\n",
    "        train_losses.append(train_loss)\n",
    "        valid_losses.append(valid_loss)\n",
    "        print(f\"Training loss   : {train_loss:.4f}\")\n",
    "        print(f\"Validation loss : {valid_loss:.4f}\")\n",
    "    return train_losses, valid_losses\n",
    "\n",
    "train_losses, valid_losses = training_Process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, folder_path, valid_losses):\n",
    "    # Create the folder if it doesn't exist\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    # Save the model state dict\n",
    "    mean_valid_loss = sum(valid_losses) / len(valid_losses)\n",
    "    torch.save(model.state_dict(), os.path.join(folder_path, f'model_loss_{mean_valid_loss:.4f}.pth'))\n",
    "\n",
    "def load_model(model, model_path):\n",
    "    # Load the model state dict\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    # Set the model to evaluation mode after loading\n",
    "    model.eval()  \n",
    "    return model\n",
    "\n",
    "# save model\n",
    "save_model(model, hyperparams.get_save_dir(), valid_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Plotting\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(valid_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Losses')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Submittion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute coverage\n",
    "# torch.unique(torch.tensor([[0, 1, 2, 3, 4, 5],[6, 7, 8, 9, 10, 11]]))  \n",
    "# --> tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prediction(model, test_dataloader, device, batch=False):\n",
    "    if batch:\n",
    "        batch = next(iter(test_dataloader))\n",
    "        session_id = batch['session_id']\n",
    "        song_ids = batch['song_id'].to(device)\n",
    "        features = batch['feature'].to(device)\n",
    "        outputs = model(song_ids, features, target_song_ids=None, use_teacher_forcing=False)\n",
    "        return session_id, outputs[:,1:]\n",
    "    else:\n",
    "        total_session_ids = torch.zeros(len(test_dataloader)*hyperparams.get_batch_size())\n",
    "        total_outputs = torch.zeros(len(test_dataloader)*hyperparams.get_batch_size(), 5)\n",
    "        # batch = next(iter(test_dataloader))\n",
    "        for idx, batch in tqdm(enumerate(test_dataloader)):\n",
    "            session_id = batch['session_id']\n",
    "            song_ids = batch['song_id'].to(device)\n",
    "            features = batch['feature'].to(device)\n",
    "            outputs = model(song_ids, features, target_song_ids=None, use_teacher_forcing=False, prediction=True)\n",
    "            total_session_ids[hyperparams.get_batch_size()*idx:hyperparams.get_batch_size()*(idx+1)] = session_id\n",
    "            total_outputs[hyperparams.get_batch_size()*idx:hyperparams.get_batch_size()*(idx+1), :] = outputs[:,1:]\n",
    "        return total_session_ids, total_outputs\n",
    "\n",
    "total_session_ids, total_outputs = generate_prediction(model, test_dataloader, hyperparams.get_device(), batch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_submittion_df = pd.DataFrame({\n",
    "    \"session_id\":total_session_ids.numpy(),\n",
    "    \"top1\":total_outputs[:,0].numpy(),\n",
    "    \"top2\":total_outputs[:,1].numpy(),\n",
    "    \"top3\":total_outputs[:,2].numpy(),\n",
    "    \"top4\":total_outputs[:,3].numpy(),\n",
    "    \"top5\":total_outputs[:,4].numpy()\n",
    "})\n",
    "# convert IDs back \n",
    "IDX_ID = {v: k for k, v in ID_IDX.items()}\n",
    "final_submittion_df['top1'] = final_submittion_df['top1'].map(IDX_ID)\n",
    "final_submittion_df['top2'] = final_submittion_df['top2'].map(IDX_ID)\n",
    "final_submittion_df['top3'] = final_submittion_df['top3'].map(IDX_ID)\n",
    "final_submittion_df['top4'] = final_submittion_df['top4'].map(IDX_ID)\n",
    "final_submittion_df['top5'] = final_submittion_df['top5'].map(IDX_ID)\n",
    "# save the final submission\n",
    "# Get today's date\n",
    "current_time = datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "save_folder = \"../submission/\"\n",
    "file_name = f'{current_time}.csv'\n",
    "final_submittion_df.to_csv(save_folder+file_name, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
